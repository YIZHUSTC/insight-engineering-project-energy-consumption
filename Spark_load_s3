import sys
import json
import configparser
import datetime
from math import radians, sin, cos, sqrt, asin
from pytz import timezone
from pyspark import SparkContext
from pyspark import SparkConf
from pyspark.sql import SparkSession
from pyspark.sql.types import (StructType, StructField, FloatType, TimestampType, IntegerType, StringType)
from pyspark.sql import SQLContext
from pyspark.sql import Row, SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import rank, col
sc =SparkContext()
sqlContext = SQLContext(sc)
from cassandra.cluster import Cluster
from cassandra.query import BatchStatement
from cassandra import ConsistencyLevel


#EC=sqlContext.read.format('csv').options(header='true', inferSchema='true').load('s3a://insightprojectsiyuguo/energycommercial/COMMERCIAL_LOAD_DATA_E_PLUS_OUTPUT.part1 (1)/USA_AK_Anchorage.Intl.AP.702730_TMY3/')
EC=sqlContext.read.format('csv').options(header='true', inferSchema='true').load('s3a://insightprojectsiyuguo/energycommercial/COMMERCIAL_LOAD_DATA_E_PLUS_OUTPUT.part1 (1)/USA_AK_Anchorage.Intl.AP.702730_TMY3/RefBldgFullServiceRestaurantNew2004_v1.3_7.1_8A_USA_AK_FAIRBANKS.csv')
EC.printSchema()

#Electricity:HVAC [kWh](Hourly)	Fans:Electricity [kWh](Hourly)	General:InteriorLights:Electricity [kWh](Hourly)	General:ExteriorLights:Electricity [kWh](Hourly)	Appl:InteriorEquipment:Electricity [kWh](Hourly)	Misc:InteriorEquipment:Electricity [kWh](Hourly)	Water Heater:WaterSystems:Gas [kWh](Hourly) 

# Add new columns to calculate total electricity, gas and water consumption
EC = EC.withColumn("Total",EC["Electricity:Facility [kW](Hourly)"]+EC["Heating:Electricity [kWh](Hourly)"]

# get total energy consumption for each column
#a1=EC.rdd.map(lambda x: float(x["Electricity:Facility [kW](Hourly)"])).reduce(lambda x, y: x+y)
#a2=EC.rdd.map(lambda x: float(x["Heating:Electricity [kW](Hourly)"])).reduce(lambda x, y: x+y)
#a3=EC.rdd.map(lambda x: float(x["Cooling:Electricity [kW](Hourly)"])).reduce(lambda x, y: x+y)
#a4=EC.rdd.map(lambda x: float(x["Electricity:Facility [kW](Hourly)"])).reduce(lambda x, y: x+y)
#a5=EC.rdd.map(lambda x: float(x["InteriorLights:Electricity [kW](Hourly)"])).reduce(lambda x, y: x+y)

# Creat DataFrame to store the result
# Total_Energy = spark.createDataFrame([("Electricity:Facility", a1), ("Heating:Electricity", a2)],["Cooling:Electricity", a3],["Electricity:Facility", a4],["CInteriorLights", a5],FloatType())

 stations_df = EC.write.format("org.apache.spark.sql.cassandra").mode('append').save()


